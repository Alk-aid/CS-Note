> 用于分类的决策树
>
> iohang

定义:

- 分类决策树模型是一种描述对实例进行分类的树形结构
- 决策树由结点（node）和有向边（directed edge）组成
- 结点有两种类型: 内部结点（internal node）和叶结点（leaf node）。内部结点表示一个特征或属性(features)，叶结点表示一个类(labels)

开发流程:

- 收集数据: 可以使用任何方法。
- 准备数据: `树构造算法` (这里使用的是ID3算法，只适用于标称型数据，这就是为什么数值型数据必须离散化。 还有其他的树构造算法，比如CART)
- 分析数据: 可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。
- 训练算法: 构造树的数据结构
- 测试算法: 使用训练好的树计算错误率
- 使用算法: 此步骤可以适用于任何监督学习任务，而使用决策树可以更好地理解数据的内在含义

构建决策树:

```python
def createBranch():
'''
此处运用了迭代的思想。 感兴趣可以搜索 迭代 recursion， 甚至是 dynamic programing。
'''
    检测数据集中的所有数据的分类标签是否相同:
        If so return 类标签
        Else:
            寻找划分数据集的最好特征（划分之后信息熵最小，也就是信息增益最大的特征）
            划分数据集
            创建分支节点
                for 每个划分的子集
                    调用函数 createBranch （创建分支的函数）并增加返回结果到分支节点中
            return 分支节点

```

优缺点:

- 优点: 计算复杂度不高，输出结果易于理解，数据有缺失也能跑，可以处理不相关特征
- 缺点: 容易过拟合
- 适用情况: 适用数据类型: 数值型和标称型

# 香农熵

1. 通过计算香农熵来 衡量 样本的混乱度; 信息越有序，信息熵越低
2. 通过信息增益来 衡量 某个特征对分类的区分程度

- S代表整个集合，前面的例子中的桶里所有的鱼的集合。
- Entropy(S)是集合的混乱度（这桶鱼的混乱程度）。A表示特征（如：是否有须），v表示特征的值（如：有须，没须）。
- Sv表示特征为A的值为v的子集（如：有须的划分一类，没须的划分为另一类，这里就划分出了两个集合了）Entropy(Sv)就是这个子集Sv的混乱程度
- 你肯定希望这个Sv越小越好，也就是越不混乱越好，也就是说你希望提出的那个分类标准有须和没须能够最大程度的区分鲫鱼和鲤鱼，这个时候得到的信息增益也是最大的
  

![image-20220808103853993](/Users/alkaid/Library/Application Support/typora-user-images/image-20220808103853993.png)

