# 1. 简介

监督学习: Learns from being given right ansewers

非监督学习: Data only comes with inputs x, but not output labels y; Algorithm has to find `structure` in the data

## 1.1 监督学习

定义:

- 必须确定目标变量的值，以便机器学习算法可以发现`特征`和`目标变量`之间的关系
- 训练样本 = 特征(feature) + 目标变量(label: 分类-离散值/回归-连续值)

需要注意的问题:

- 偏置方差权衡
- 功能的复杂性和数量的训练数据
- 输入空间的维数
- 噪声中的输出值

主要场景:

- `Regression`: Predict a number which from infinitely many possible outputs, 主要用于预测数值型数据

- `Classification`: predict categories, 将实例数据划分到合适的类别中

<img src="/Users/alkaid/Library/Application Support/typora-user-images/image-20220807180429868.png" alt="image-20220807180429868" style="zoom:50%;" />

## 1.2 非监督学习

- `Clustering`: Group similar data points together

- `Dimensionality reduction`: Compress data using fewer numbers

- `Anomaly detection`: Find unusual data points

## 1.3 术语

- m: 代表训练集中实例的数量
- x: input / feature
- y: output / target
- $$y-hat$$: y的估计值
- (x,y): single training example
- $$(x^{(i)},y^{(i)})$$: 代表第 i 个观察实例：其中$$x^{(i)}$$ 代表第i个输入变量, $$y^{(i)}$$代表第i个目标变量
- h: 代表学习算法的解决方案或函数，也称为假设（hypothesis）

拟合程度:

- 欠拟合（Underfitting）: 模型没有很好地捕捉到数据特征，不能够很好地拟合数据，对训练样本的一般性质尚未学好。类比，光看书不做题觉得自己什么都会了，上了考场才知道自己啥都不会。
- 过拟合（Overfitting）: 模型把训练样本学习“太好了”，可能把一些训练样本自身的特性当做了所有潜在样本都有的一般性质，导致泛化能力下降。类比，做课后题全都做对了，超纲题也都认为是考试必考题目，上了考场还是啥都不会。

模型:

- 分类问题 —— 说白了就是将一些未知类别的数据分到现在已知的类别中去。比如，根据你的一些信息，判断你是高富帅，还是穷屌丝。评判分类效果好坏的三个指标就是上面介绍的三个指标: 正确率，召回率，F值。
- 回归问题 —— 对数值型连续随机变量进行预测和建模的监督学习算法。回归往往会通过计算 误差（Error）来确定模型的精确性。
- 聚类问题 —— 聚类是一种无监督学习任务，该算法基于数据的内部结构寻找观察样本的自然族群（即集群）。聚类问题的标准一般基于距离: 簇内距离（Intra-cluster Distance） 和 簇间距离（Inter-cluster Distance） 。簇内距离是越小越好，也就是簇内的元素越相似越好；而簇间距离越大越好，也就是说簇间（不同簇）元素越不相同越好。一般的，衡量聚类问题会给出一个结合簇内距离和簇间距离的公式。

模型指标:

- 正确率 —— 提取出的正确信息条数 / 提取出的信息条数
- 召回率 —— 提取出的正确信息条数 / 样本中的信息条数
- F 值 —— 正确率 * 召回率 * 2 / （正确率 + 召回率）（F值即为正确率和召回率的调和平均值）

# 2. Liner Regression Model

## 2.1 Cost Function

定义: 是定义在整个训练集上的，是所有样本误差的平均，也就是损失函数的平均

作用: 用于找到最优解, Cost Function最小就是最优解

线性回归方程的代价函数: 
$$
J(w,b) = \frac{1}{2m} \sum\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2 \tag{1}
$$


## 2.2 Gradient descent

作用: 在无约束优化问题时, 求解机器学习算法的模型参数;比如求代价函数的参数使得满足局部最小
$$
w = w - \alpha\frac{\partial }{\partial w}J(w,b)
$$

$$
b = b - \alpha\frac{\partial }{\partial b}J(w,b)
$$

梯度下降: 

- start with some w,b
- keep changing w,b to reduce J(w,b) Util we settle at or near a minimum
- $$\alpha $$: learning rate, 决定下降的步长
- 导数: 决定从哪个方向下降

<img src="/Users/alkaid/Library/Application Support/typora-user-images/image-20220805181823061.png" alt="image-20220805181823061" style="zoom: 25%;" />

 
