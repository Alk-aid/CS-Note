# 1. 前置知识

1. 分词的发展阶段: 字面的匹配, 词汇的匹配 语义的匹配
2. 分词: 把每个词都分配一个唯一id，我们叫这个词的token
3. 词袋模型: 通俗来说: 把一个文档分词得到的一堆token放到一个袋子里，用这个袋子来表示这个文档

4. 向量化: 

- 文本本身不可计算, 所以使用数学工具进行数学化, 也就是变成向量
- 向量可以表示为空间中的一个点
- 先有了文本，文本变成了向量，再有了空间，向量变成了空间的点，那么我们通过求两个点之间的距离，就求得了两个文档的相似性

5. 向量化的方法: TF-IDF方法

- TF-IDF的作用就是保留词在文档中的权重信息，这就相当于保留了文本的信息
- 每一个向量就是一个前面提到的词袋

6. 向量相似度的计算

- 欧式距离: 直接计算这两个点的欧式距离
- 余弦相似度距离: 求两个向量之间的夹角，这个叫余弦相似性

7. 最简单的字面匹配: JaccardSimilarity

- 两个集合的交集除以两个集合的并集，所得的就是两个集合的相似度
- 把所有的文档进行两两比较得到相似性, 然后进行排序

8. 词向量: word2vector





# 1. 匹配方式

> 同义词分析
>
> 文本相似度
>
> 主题模型
>
> 词向量分析
>
> 语义归一化
>
> 文本分类
>
> 

传统文本匹配方式: `BoW、VSM、TF-IDF、 BM25、Jaccord、SimHash`等算法;

- 基于词汇层面进行匹配

主题模型: 无监督技术

- 语义分析技术（`Latent Sementic Analysis, LSA`）: 将语句映射到等长的低维连续空间，可在此隐式的潜在语义空间上进行相似度计算
- LDA

神经语义匹配模型:

- 基于神经网络训练出的`Word Embedding`来进行文本匹配计算, 所得的词语向量表示的语义可计算性进一步加强
- 但是只利用无标注数据训练得到的`Word Embedding`在匹配度计算的实用效果上和主题模型技术相差不大。他们本质都是基于共现信息的训练。
- 另外，`Word Embedding`本身没有解决短语、句子的语义表示问题，也没有解决匹配的非对称性问题

# 2. 主题模型

