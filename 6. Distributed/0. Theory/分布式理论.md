# 1. 认识分布式系统

什么是分布式系统

- 定义：一个 组件分布在不同的，联网的计算机上, 组件之间通过传递消息进行通信和协调，共同完成一个任务
- 特点：多进程, 不共享操作系统, 不共享时钟
- 优点: 高性能, 可扩展性, 高可用性
- 缺点： 缺乏全局时钟：很难定义两个事件的先后顺序

拜占庭将军模型: 

- 多个将军率领一支军队想要占领一座城市, 军队之间只能通过信使进行交流
- 军队的动作只有进攻和撤退, 将军们必须对行为达成一致才算成功
- TODO

分布式系统中的同步 和 异步：

- 同步：一个消息的响应时间在一个有限且已知的时间范围内
- 异步：一个消息的响应时间是无限的，无法知道一条消息什么时候会到达
- 部分同步：假设系统在大部分时间都是同步的，但偶尔会因为故障转变为异步系统

消息传递语义：

- At Most Once：消息最多传递一次，消息可能丢失，但不会重复
- At Least Once: 系统保证每条消息至少会发送一次，消息不会丢失，但在有故障的情况下可能导致消息重复发送
- Exactly Once: 消息只会被精确传递一次, `难以实现`

# 2. 分布式数据基础

## 2.1 分区(Partition)

> 提高可用性

1. 定义： 将一个数据集拆分为多个较小的数据集, 同时将存储和处理这些较小数据集的责任分配给分布式系统中的不同节点
2. 分类：水平分区 和 垂直分区

- 垂直分区: 对表的列进行拆分; 可以将不常用的列 或者包含text等大对象的列进行拆分
- 水平分区(Sharding,分片): 对表的行进行拆分

3. 水平分区算法：计算某个数据应该划分到哪个分区上

- `范围分区`(Range Partitioning): 根据指定的关键字将数据集拆分为若干连续的范围, 每个范围存储到一个单独的节点上
  - 优点: 实现简单；通过修改范围边界减少范围数据, 能够有效地重新分区
  - 缺点: 无法使用`分区键` 之外的其他关键字进行范围查询； 可能产生数据分布不均匀 或 请求流量不均匀的情况
- `哈希分区`(Hash Partitioning): 将制定的关键字通过hash函数, 根据得到的值来决定该数据集的分区
  - 优点: 数据的分布是随机的，分布较为均匀
  - 缺点: 不支持范围查询；当机器数增加或者减少时，那么映射关系就发生了改变(举例)，从而导致大量缓存的key失效，造成缓存雪崩
- `一致性哈希`(Consistent Partitioning):特殊的hash分区算法

```java
package leetcode.editor.cn;

import org.omg.CORBA.MARSHAL;

import java.util.*;

public class ConcurrentHash {
    public static void main(String[] args) {
        MachineArray machineArray = new MachineArray();

        Machine[] nodes = {
                new Machine("Node--> 1"),
                new Machine("Node--> 2"),
                new Machine("Node--> 3")
        };

        for (Machine node : nodes) {
            machineArray.addMachine(node);
        }

        Data[] objs = {
                new Data("1"),
                new Data("2"),
                new Data("3"),
                new Data("4"),
                new Data("5")
        };

        for (Data obj : objs) {
            machineArray.put(obj);
        }

        validate(machineArray, objs);
    }

    private static void validate(MachineArray nodeArray, Data[] objs) {
        for (Data obj : objs) {
            System.out.println(nodeArray.get(obj));
        }

        nodeArray.addMachine(new Machine("anything1"));
        nodeArray.addMachine(new Machine("anything2"));

        System.out.println("========== after  =============");

        for (Data obj : objs) {
            System.out.println(nodeArray.get(obj));
        }
    }

}

// Data对象
class Data {
    String value;

    Data(String value) {
        this.value = value;
    }

    @Override
    public int hashCode() {
        return Objects.hash(value);
    }

    @Override
    public String toString() {
        return "Data{" +
                "value='" + value + '\'' +
                '}';
    }
}

// 机器对象
class Machine {

    Map<Integer, Data> map = new HashMap<>();
    String name;

    Machine(String name) {
        this.name = name;
    }

    public void putObj(Data obj) {
        map.put(obj.hashCode(), obj);
    }

    Data getObj(Data obj) {
        return map.get(obj.hashCode());
    }

    @Override
    public int hashCode() {
        return name.hashCode();
    }
}

// 机器集合
class MachineArray {

    /** 按照 键 排序*/
    TreeMap<Integer, Machine> machines = new TreeMap<>();

    int size = 0;

    public void addMachine(Machine machine) {
        machines.put(machine.hashCode(), machine);
    }

    // put 方法中，对象如果没有落到缓存节点上，就找比他小的节点且离他最近的
    void put(Data obj) {
        int objHashcode = obj.hashCode();
        Machine machine = machines.get(objHashcode);
        if (machine != null) {
            machine.putObj(obj);
            return;
        }
        // 找到比给定 key 大的集合
        SortedMap<Integer, Machine> tailMap = machines.tailMap(objHashcode);
        // 找到最小的节点
        int nodeHashcode = tailMap.isEmpty() ? machines.firstKey() : tailMap.firstKey();
        machines.get(nodeHashcode).putObj(obj);
    }

    Data get(Data obj) {
        Machine machine = machines.get(obj.hashCode());
        if (machine != null) {
            return machine.getObj(obj);
        }

        // 找到比给定 key 大的集合
        SortedMap<Integer, Machine> tailMap = machines.tailMap(obj.hashCode());
        // 找到最小的节点
        int nodeHashcode = tailMap.isEmpty() ? machines.firstKey() : tailMap.firstKey();
        return machines.get(nodeHashcode).getObj(obj);
    }
}


```



4. 一致性哈希具体介绍：

- 流程: 通过hash环来实现，hash环的大小是$$2^{32}-1$$;；首先将服务器(用IP或者主机名)通过hash映射放到hash环上；然后再将数据也通过hash映射放到hash环上面；然后在hash环上进行顺时针查找距离这个对象最近的一个服务器，让这个服务器缓存这个数据即可
- 优势: 良好的扩展性；当增加了服务器或者减少了，会导致一小部分缓存失效，大部分的缓存key仍然可用
- 缺点：不额外存储数据, 依然不支持范围查询
- hash偏斜问题: 如果服务器没有均匀的分布在hash环上，导致缓存大部分分布在一个服务器上，导致缓存不均匀，使得服务器没有得到良好的使用，一旦缓存失效，容易引起系统的崩溃
- hash偏斜解决: 引入虚拟结点, 将一个真实结点映射为多个虚拟结点。从而缓存读写的流程变成(先找到虚拟结点，再找到真实结点然后进行读取)

5. 分区的挑战: 

- 不好实现事务
- 垂直查询存在多join查询, 很低效

## 2.2 复制(Replication)

1. 定义：将同一份数据冗余存储在多个节点上, 节点通过网络来同步数据, 使之保持一致

2. 优点：提高可用性和安全性; 减少往返时间(用户从最近的副本中拿数据); 增加吞吐量(多个提供读写的机器)

3. 缺点：保持多副本的一致性，增加了复杂性
4. 分类：单主(Single-Master)复制; 多主(Multi-Master)复制; 无主(Leaderless)复制

5. 单主复制: 主从复制, 一主多从

- 写请求必须发送给主节点; 从节点只能处理读请求, 并从主节点同步最新的数据
- `同步复制`: 主节点执行完写请求后, 必须等所有的从节点都执行完毕, 并收到确认信息后才能回复客户端写入成功; `存在性能低下的情况`
- `异步复制`: 主节点执行完写请请求后, 立即将结果返回给客户端, 无须等待其他副本是否写入完成；`存在数据不一致的情况`
- `半同步复制`: 主节点只要等待至少一个从节点返回完成信息即可, 这样保证至少有两个节点拥有最新的数据
- 单主复制优点：只有主节点执行并发写操作,能够保证操作的顺序性, 更加容易支持事务操作
- 单主复制缺点：当主节点宕机, 如果是自动切换的情况, 可能有两个从节点升级为主节点, 导致脑裂

5. 多主复制: 多个节点充当主节点的数据复制方式

- 问题: 多主复制不止一个节点处理写请求, 且网络存在延迟, 这就意味着节点可能会对某些请求的正确顺序产生分歧
- `客户端解决冲突`: 用户自行解决, 将冲突的数据全部返回给客户端, 客户端选择合适的数据返回给系统; 如购物车出现了被删除的物品等
- `最后写入胜利(Last Write Wins)`: 让系统中的每个节点为每个写入请求标记上一个唯一的时间戳或者ID, 冲突发生时选择最新的时间戳; `但是分布式系统难有一个统一的全局时间`
- `因果关系追踪`: 使用算法来跟踪不同请求之间的因果关系
- 多主复制优点: 增加主节点容错性; 多主节点执行写请求, 分担负载的压力
- 多主复制缺点: 复杂性太高 远超他的好处

5. 无主复制: 没有主节点

- 客户端向多个节点发送写请求, 一旦得到其中一些节点的确认响应, 就认为这次写请求成功
- 客户端的读请求也会向多个节点读, 然后获取节点上的数据的版本号, 来决定使用哪一个

## 2.3 CAP & BASE

> CAP理论十二年回顾: 规则变了

`CAP理论`: 在一个异步网络环境中, 对于一个分布式读写存储系统, 不可能同时满足一致性(Consistency), 可用性(Availability), 分区容错性(Partition tolerance)

| 选项            | 描述                                                         |
| --------------- | ------------------------------------------------------------ |
| C(Consistence)  | 服务器层面：数据在多个副本之间能够保持一致的特性（强一致性)<br>客户端层面：用户访问分布式系统中的任意节点，得到的数据必须一致(强一致性) |
| A(Availability) | 每次请求都能获取非错误的响应, 但不能保证获取的数据是最新的   |
| P(Partition)    | 不同的节点分布在不同的子网络中，由于一些特殊的原因，这些子节点之间出现了网络不通的状态，但他们的内部子网络是正常的。从而导致了整个系统的环境被切分成了若干个孤立的区域 |
| T（Tolerance)   | 系统在遇到任何网络分区问题时，仍能对外提供服务，除非整个网络都故障了 |

CAP难以同时满足，只能由 CP 或者 AP模式

- 分布式系统节点通过网络连接，必然有网络断开的风险，一定会出现分区问题（P）
- 放弃A：当遇到网络分区或者其他故障时，在此期间无法对外提供正常服务
- 放弃C：放弃C不是放弃一致性，而是放弃强一致性，保留最终一致性

---

`BASE理论`: 是对CAP的一种解决思路（AP方案的补充），包含三个思想：

- `Basically Available （基本可用）`：分布式系统在出现故障时，允许损失部分可用性，即保证核心可用
  - **响应时间上的损失**: 正常情况下，处理用户请求需要 0.5s 返回结果，但是由于系统出现故障，处理用户请求的时间变为3s 
  - **系统功能上的损失**：正常情况，用户可以使用系统全部功能,但是由于系统访问量突然剧增,系统的部分非核心功能无法使用(降级处理)

- `Soft State（软状态）`：**在一定时间内，允许出现中间状态，比如临时的不一致状态。**

- `Eventually Consistent（最终一致性）`：虽然无法保证强一致性，但是在软状态结束后，最终达到数据一致

> Redis的主从数据就是异步的，因此不满足强一致性，；而是保证最终一致性

最终一致性的实现方案

- **读时修复** : 在读取数据时，检测数据的不一致，进行修复
- **写时修复** : 在写入数据，检测数据的不一致时，进行修复
- **异步修复** : 这个是最常用的方式，通过定时对账检测副本数据的一致性，并修复

## 2.4 一致性模型

1. `线性一致性`: 也叫做强一致性； 保证了C 就保证不了A

- 非严格定义: 分布式系统的所有操作看起来都是原子的, 看起来好像只有一个节点
- 严格定义: 给定一个执行历史, 根据并发操作可以扩展为多个顺序历史, 只要从中找到一个合法的顺序历史, 那么这个执行历史就是线性一致的
- 代价: 实现强一致性的同步原语会增加系统开销; 强一致性最困难的是需要一个全局时钟, 这样才能知道每个节点事件发生的时间和全局顺序
- 现代CPU在访问内存时不保证线性一致性

2. `顺序一致性`: 保证了C 就保证不了A

- 只要球同一个客户端(进程)的操作在排序后保持先后顺序不变, 但不同客户端之间的先后顺序是可以任意改变的
- 和强一致性的区别: 没有全局时钟的限制, 不要求不同客户端之间的顺序一致, 只关注局部的顺序
- 举例: 不关心所有朋友的帖子的顺序, 但对于具体一个朋友, 帖子的顺序性要求保证 
- 现代CPU在默认情况也不保证顺序一致性

3. `因果一致性`: 可以容忍一部分节点发生故障, 还未出现故障的节点仍然可用, 但客户不能将请求发送到不可用的副本节点

- 必须以相同的顺序看到因果相关的操作, 而没有因果关系的并发操作可以被不同的进程以不同的顺序观察到
- 比如发帖和回贴, 发帖一定早于回贴

4. `弱一致性`：这种一致性级别约束了系统在写入成功后，不承诺立即可以读到写入的值，也不承诺多久之后数据能够达到一致，但会尽可能地保证到某个时间级别（比如秒级别）后，数据能够达到一致状态

4. `最终一致性`: 最终一致性是弱一致性的一个特例，系统会保证在一定时间内，能够达到一个数据一致的状态。

## 2.5 总结

分布式系统的设计难点: 

- 出发点是为了高性能: 因为数据密集, 单机不足以支撑所有的需求; 所有需要对数据进行`分区`, 然后放到多台服务器上面
- 容错性的需求 :因为服务器的增多, 使得机器出现故障的现象大大增加; 为了提高容错性, 需要对数据进行`复制` 
- 一致性的需求: 因为复制往往会导致数据不一致, 所以我们需要提高`一致性`, 而提高一致性就代表着更低的性能，和出发点违背

# 3. 分布式共识

1. 共识(Consensus)和一致性(Consistency)

- 共识: 侧重于研究分布式系统中的节点达到共识的过程和算法
- 一致性: 研究副本最终的稳定状态

2. 共识的准确描述

- 一个分布式系统包含n个进程, 每个进程都有初始值, 进程之间互相通信, 设计一种算法使得尽管出现故障, 但进程之间仍能协商出某个不可撤销的最终决定
- 每次执行都满足三个性质: 终止性, 协定性, 完整性
- 终止性(Termination): 所有正确的进程最终都会认同某一个值
- 协定性(Agreement): 所有正确的进程认同的值都是同一个值
- 完整性(Integrity): 如果正确的进程都提倡同一个值v, 那么任何正确进程的最终决定值一定是v

3. FLP不可能定理

- 定义: 在一个完全异步系统中, 即使只有一个节点出现了故障, 也不存在一个算法使系统达到共识
- 举例: 进程可以任意时间返回响应, 因此无法分辨进程是速度慢还是崩溃了, 从而无法在有限时间能达到共识
- 含义: 共识算法不可能同时实现三点: `安全性`, `活性`,`容错性`

4. 安全性 和 活性: 分布式共识算法所需要具备的两个属性

- 安全性(Safety): 所有正确的进程都认同同一个值
- 活性(Liveness): 分布式系统最终会认同某一个值

## 3.1 Raft

1. Raft定义: 保证日志完全相同地复制到多台服务器上, 以实现状态机复制的算法; R{eliable|eplicated|edundant} And Fault-default
2. Raft服务器的状态: 

- `Leader`：负责发起心跳，响应客户端，创建日志，同步日志; 同一时刻最多只有一个正常的leader
- `Follower`：接受 Leader 的心跳和日志同步数据，投票给 Candidate;  只响应来自Leader和Candidate的请求, 不会发送任何请求
- `Candidate`: 用来选举出新的leader, 处于leader 和 follower的中间状态

3. Term

- Raft算法选出leader就意味着进入一个新的任期(Term), Term实际上是一个逻辑时间；每个term都由一个数字来表示任期号, 初始值为0, 单调递增且永不为0
- Raft将分布式系统中的时间划分为一个个不同的任期来解决时序问题; Raft只使用最新任期的信息
- Term分为两部分: 任期开始的选举过程 和 任期正常运行的过程; 
- 每个服务器维护一个currentTerm变量表示服务器知道的最新的term号; currentTerm必须持久化存储

4. RPC调用

- RequestVote RPC: 用于领导者选举
- AppendEntries RPC: leader用来日志复制 和 发送心跳

### 3.1.1 领导者选举

1. 领导者选举的前提:

- 如果一个follower在规定时间(electionTimeout)没收到任期更大的空的 AppendEntries RPC请求(心跳检测), 那么就开始新的一轮选举

2. 领导者选举的过程:

- 节点变为candidate, 目标获得超过半数的选票, 让自己成为新的leader
- 增加自己当前任期变量currentTerm
- 给自己投一票
- 并行向系统其他的节点发送RequestVote RPC索要选票

3. 投票的结果

- 获得超过半数选票, 该节点成为新的`leader`, 然后每隔一段时间向其他节点发送AppendEntries消息作为心跳, 来维持自己的leader身份
- 收到来自领导者的心跳RPC请求, 说明已经有leader了, 那么该节点变为`follwer`
- 经过选举超时时间以后, 前面两种情况都没发生, 也没其他节点获胜, 那么回到第二步, 开启新一轮选举

4. 如何保证安全性: 一个term内只有一个leader被选举出来

- 每个节点在同一term内只能投一次票, 给第一个满足条件的RequestVote请求(`TODO:如何算满足条件`)
- 只有获得超过半数的选票才能成为领导者

5. 如何保证活性: 确保系统最终能选出一个leader

- 问题：如果选举同一时间开始, 然后瓜分选票, 没有任何一个节点过半, 又同一时间超时; 然后开启新一轮循环
- 解决: 节点随机选择超时时间,, 选举超时时间通常在[T,2T] (例如150-300)

```go
// Raft节点的结构体
type Raft struct {
  mu sync.Mutex
  // 服务器状态
  state int
  // 服务器已知的最新任期
  currentTerm int
  // 当前任期投票给哪个服务器
  votteFor int
  heartbeatTime time.TIme
  // 其他信息
  // 状态机的日志
  log[] LogEntry
}
```

```go
type RequestVoteArgs struct {
  // 候选者任期
  Term int
  // 候选者id
  CandidateId int
}
type RequestVoteReply struct {
  // 处理请求节点的任期号, 用于候选者更新自己的任期
  Term int
  // 候选者获得选票时为true
  VoteGranted bool
}
```

```go
// 判断请求是否合理
func (rf *Raft) RequestVote(args *RequestVoteArgs, reply *RequestVoteReply) {
  rf.mu.Lock()
  defer rf.mu.UnLock()
  
  reply.Term = rf.currentTerm
  reply.voteGranted = false
  if args.Term < rf.currentTerm {
    return
  }
  // 如果收到来自更大任期的请求, 则更新自己的curretTerm, 转为follwer
  if args.Term > rf.curretTerm {
    rf.currentTerm = args.Term
    rf.state = Follower
    rf.votedFor = -1
  }
  
  if rf.votedFor == -1 || rf.votedFor == args.CandidateId {
    	rf.votedFor = args.CandidateId
	    reply.voteGranted = true
	    rf.heartbeatTime = time.Now()
  }
  return
}
```

### 3.1.2 日志复制

日志格式: 通过索引 和 任期号唯一标识一条日志记录

- index: 表示该日志条目在整个日志中的位置
- 任期号: 日志条目首次被leader创建时的任期
- 命令: 应用于状态机的命令

日志复制流程:

- 客户端向leader发送命令, 希望该命令被所有状态机执行
- leader先将命令追加到自己的日志中, 确保日志持久化存储
- leader 并行向其他节点发送AppendEntries消息, 等待响应
- 如果收到超过半数节点的回应, 则认为该日志`已提交`; 然后leader执行这个命令, 并向客户端返回响应
- 如果followe宕机或者请求超时, 那么leader将尝试反复发送AppendEntries消息

raft 保证以下两个性质：

- 在两个日志里，有两个 entry 拥有相同的 index 和 term，那么它们一定有相同的 cmd，且 它们前面的 entry 也一定相同
- 如果给定的记录已提交, 那么前面所有的记录也提交了

一致性检查: Raft会通过AppendEntries消息来检测之前的日志entry

- 每个AppendEntries消息请求包含 新日志entry之前一个日志的索引(preLogIndex)和任期(preLogTerm)
- follower收到请求后, 会检查自己最后一个日志的索引和任期是否对的上preLogIndex, preLogTerm

```go
type LogEntry struct {
	  Index int 
  	Term	int
	  Command interface{}
}
type AppendEntriesArgs struct {
  	Term 	int
  	LeaderId 	int
  	preLogIndex int
  	preLogTerm 	int
  	// 需要复制的日志条目, 心跳检测时Entries为空
  	Entries	[]LogEntry
  	// leader已经提交的最大的日志索引, 用于follower提交
  	LeaderCommit int
}
type AppendEntriesReply struct {
  	Term int
  	Succeess bool
}
```

```go
//TODO
func (rf *Raft) AppendEntries(args *AppendEntriesArgs, reply *AppendEntriesReply)
```

### 3.1.3 领导者更替

前提:

- Raft假定leader的日志始终是对的, 所以leader要在运行过程中让所有follower的日志都与其匹配

## 3.2 Paxos(ToDo)

定义：基于`消息传递`且具有`高度容错特性`的`一致性算法`

前提：消息可能会出现延迟，丢失，重复，但是消息不会被篡改

- 因为大多数系统都在同一局域网，所以消息被篡改很少
- 由于硬件和网络原因造成的消息不完整可以通过校验算法避免

目标：最终只有一个提案会被选中，当提案被选择后，进程最终也能获取到被选中的提案

分布式系统中最重要的是安全性 和 活性

- 安全性：保证永远都不会发送的事情
- 活性：指最终一定会发生的事情

### 3.2.1 推导

P1: 一个Acceptor必须批准它收到的第一个提案,一个提案被选定需要半数以上的Acceptor批准（一个Acceptor能批准多个提案）

P2:  如果一个提案[M0, V0]被选定后，那么所有编号比M0更高的，且被选定的提案，其Value也是V0

P2a：如果一个提案[M0, V0]被选定后，那么所有编号比M0更高的，且被Acceptor批准的提案，其Value也是V0

P2B: 如果一个提案[M0, V0]被选定后，那么之后任何Proposer产生的编号更高的提案，其Value值都必须为V0

P2C：如果提案[$M_n$, $V_n$]被提出，那么存在一个半数以上的Acceptor组成的集合S，满足以下两个条件中的任意一个：

- S中每个Acceptor都没有接受过编号小于$$M_n$$的提案。
- S中Acceptor接受过的最大编号的提案的value为$V_n$。

### 3.2.2 Proposer生成提案：

为了满足P2C，Proposer生成提案之前

- 应该先去**『学习』**已经被选定或者可能被选定的value，然后以该value作为自己提出的提案的value。
- 如果没有value被选定，Proposer才可以自己决定value的值
- 这个学习的阶段是通过一个**『Prepare请求』**实现的

Perpare请求(**提案生成算法**)

- `Prepare请求`: Proposer选择一个**新的提案编号N**，然后向**某个Acceptor集合**（半数以上）发送请求，要求该集合中的每个Acceptor做出如下响应（response）
  - 向Proposer承诺保证**不再接受**任何编号**小于N的提案**
  - 如果Acceptor已经接受过提案，那么就向Proposer响应**已经接受过**的编号小于N的**最大编号的提案**
- `Accept请求`:
  - 如果Proposer收到了**半数以上**的Acceptor的**响应**，那么它就可以生成编号为N，Value为V的**提案[N,V]**。这里的V是所有的响应中**编号最大的提案的Value**。
  - 如果所有的响应中**都没有提案**，那 么此时V就可以由Proposer**自己选择**
  - 生成提案后，Proposer将该**提案**发送给**半数以上**的Acceptor集合，并期望这些Acceptor能接受该提案
  - 注意：此时接受Accept请求的Acceptor集合**不一定**是之前响应Prepare请求的Acceptor集合

### 3.2.3 Acceptor批准提案

Acceptor**可以忽略任何请求**（包括Prepare请求和Accept请求）而不用担心破坏算法的**安全性**.

因此，我们这里要讨论的是什么时候Acceptor可以响应一个请求,我们对Acceptor接受提案给出如下约束

> P1a：一个Acceptor只要尚**未响应过**任何**编号大于N**的**Prepare请求**，那么他就可以**接受**这个**编号为N的提案**

- 如果Acceptor收到一个编号为N的Prepare请求，在此之前它已经响应过编号大于N的Prepare请求。根据P1a，该Acceptor不可能接受编号为N的提案。因此，该Acceptor可以忽略编号为N的Prepare请求。当然，也可以回复一个error，让Proposer尽早知道自己的提案不会被接受。

一个Acceptor**只需记住**：1. 已接受的编号最大的提案 2. 已响应的请求的最大编号

### 3.2.4 流程总结

Paxos算法分为**两个阶段**。具体如下：

**阶段一：**

- Proposer选择一个**提案编号N**，然后向**半数以上**的Acceptor发送编号为N的**Prepare请求**。
- 如果一个Acceptor收到一个编号为N的Prepare请求，且N**大于**该Acceptor已经**响应过的**所有**Prepare请求**的编号，那么它就会将它已经**接受过的编号最大的提案（如果有的话）**作为响应反馈给Proposer，同时该Acceptor承诺**不再接受**任何**编号小于N的提案**。

**阶段二：**

- 如果Proposer收到**半数以上**Acceptor对其发出的编号为N的Prepare请求的**响应**，那么它就会发送一个针对**[N,V]提案**的**Accept请求**给**半数以上**的Acceptor。注意：V就是收到的**响应**中**编号最大的提案的value**，如果响应中**不包含任何提案**，那么V就由Proposer**自己决定**。
- 如果Acceptor收到一个针对编号为N的提案的Accept请求，只要该Acceptor**没有**对编号**大于N**的**Prepare请求**做出过**响应**，它就**接受该提案**。

![Paxos算法流程](http://aikaid-img.oss-cn-shanghai.aliyuncs.com/img/1752522-44c5a422f917bfc5.jpg)

### 3.2.5 Learner学习被选定的value

![幻灯片17.png](http://aikaid-img.oss-cn-shanghai.aliyuncs.com/img/1752522-0fab48ed2bdf358a.png)

### 3.2.6 如何保证Paxos算法的活性

![幻灯片18.png](http://aikaid-img.oss-cn-shanghai.aliyuncs.com/img/1752522-28b18dd606777074.png)







# 4. 分布式事务

分布式事务的ACID:

- 持久性: 和单点没区别, 只需要在向客户端响应之前, 将数据存在日志中即可
- 一致性: 一般不讨论, 因为实现一致性是由数据库和应用程序一起保证的
- 原子性: 原子提交
- 隔离性: 并发控制

## 4.1 原子提交---实现原子性

- `出现原因`：每一个节点能知道自己的事务操作的结果，却无法知道其他分布式节点的操作结果；所以当一个事务要跨越多个分布式节点时，为了保证ACID，所以需要引入一个`协调者`(Coordinator)
- `协调者`：负责调度参与者的行为，并最终决定参与者是否要把事务真正进行提交
- `目的`：为了保证原子性 和 一致性

### 4.1.1 2PC(第一选择)

> 2PC就是将事务的提交过程分为了两个阶段来进行处理

1. 阶段一：提交事务请求(投票阶段)

- `事务询问`：协调者向所有参与者并行发送准备消息，询问是否可以执行事务commit操作，并等待参与者的响应
- `执行事务`：参与者检查执行事务所需的资源(如权限校验, 上锁), 准备好后执行事务，并将undo 和 redo信息记录到事务日志中去
- `参与者反馈`：如果条件检验失败 或者 执行失败 返回 No; 否则返回 Yes

2. 阶段二：执行事务提交

- 如果收到的反馈都是YES，则执行事务提交 
  - 协调者向所有参与者发送提交请求
  - 参与者收到提交请求后，会执行 commit 操作，然后执行完后释放占用的资源(释放锁)，并记录操作日志, 后向协调者发送ACK
  - 协调者收到所有 ACK 后，确定事务完成
- 如果至少有一个返回了 No 或者超时，则中断事务
  - 协调者向参与者发送中止(rollback)请求
  - 参与者收到中止请求后，会利用 undo 信息执行事务回滚，回滚完释放占用的资源，然后向协调者发送Ack消息
  - 协调者收到所有的ACK后，完成事务中断

3. 故障情况一: `同步阻塞`

- 问题: 第一阶段, 参与者在回复协调者前发生了故障, 那么协调者由于没收到all OK, 只能一直等
- 解决方式: 协调者设置一个超时时间, 超过了就选择中止事务

4. 故障情况二: `单点问题`

- 问题: 第一阶段, 如果协调者在向参与者发送请求以后就故障, 那么就存在单点故障, 整个流程被阻塞住

5. 故障情况三: `网络分区`

- 问题: 协调者发送了部分 提交请求 就发生网络分区, 导致剩下的无法接收到commit请求, 也就是说只有部分事务提交了事务, 会导致数据不一致的情况

6. 优点：原理简单， 实现方便

7. 缺点：：**二阶段提交协议没有设计较为完善的容错机制，任意一个节点是失败都会导致整个事务的失败**

### 4.1.2 3PC

1. 出现原因:

- 2PC存在单点故障, 所以希望当协调者失效后, 能有一个节点来充当协调者;  但是需要让该参与者知道其他参与者状态才有资格成为协调者
- 增加一个PreCommit阶段, 让协调者将第一阶段的结果发送给所有参与者

2. CanCommit：

- `事务询问`：协调者并向向所有的参与者发送准备消息，询问是否可以执行事务操作，并开始等待响应
- `参与者响应`：参与者收到请求后，如果认为自身可以执行事务则返回YES

3. PreCommit：

- 如果都返回YES，则执行事务预提交；
  - 协调者向参与者发送preCommit请求, 询问是否可以执行并提交事务, 并等待响应
  - 参与者检查执行事务所需的资源(如权限校验, 上锁), 准备好后执行事务，并将undo 和 redo信息记录到事务日志中去； 执行成功后向协调者返回Ack响应，同时等待指令
- 如果至少有一个返回了No或者超时，则中断事务；
  - 协调者向所有参与者发送abort请求；无论是超时还是收到abort，参与者都中断事务

4. do Commit：

- 执行提交：
  - 收到了所有的Ack响应，然后向所有的参与者发送doCommit；
  - 参与者收到doCommit请求后，会执行Commit操作，然后执行完后释放占用的资源，参与者完成Commit后向协调者发送ACK
  - 协调者收到所有ACK后，完成事务
- 中断事务：
  - 协调者向所有参与者发送abort请求；
  - 参与者收到abort请求后，会利用Undo信息执行事务回滚，回滚完释放占用的资源，然后向协调者发送Ack消息
  - 协调者收到所有的ACK后，完成事务中

5. 优点:

- `无单点故障, 造成阻塞`: 3pc说非阻塞协议, 即使协调者发生故障,  参与者会显出一个新的协调者来推进事务的执行

6. 缺点：

- `网络分区`: 如果部分参与者收到了 preCommit 消息后, 出现了网络分区, 同时协调者故障; 那么两边会各自选出新的协调者, 那么一边可能提交事务, 另一边可能终止事务

- `延时高`: 因为一次事务至少需要三轮消息往返才能完成

>2PC是第一选择

### 4.1.3 Paxos提交(实现难度大)

## 4.2 并发控制---实现隔离性

### 4.2.1 PCC

> PCC: Pessimistic Concurrency Control
>
> 2PL: Two-phase Locking

和Mysql的锁same

### 4.2.2 OCC

>OCC: Optimistic Concurrency Control

1. 基于检查的并发控制(Validation-Based Concurrency Control)

- Read: 将事务涉及的数据复制一份副本, 放到私有空间, 之后读操作读取的 是私有副本中的数据; 写操作被记录到私有空间的临时文件
- Validation: 检查在此期间是否与其他事务产生冲突, 有冲突 则 终止事务
- Write: 校验成功泽江私有空间中的数据写入到数据库中

2. 基于时间戳的并发控制(Timestamp-Based Concurrency Control)

- TODO

### 4.2.3 MVCC

多版本并发控制是在以上三种实现方式的基础上增加`多个版本`

- 多版本两阶段锁
- 多版本乐观并发控制
- 多版本时间戳排序





# 5. 分布式时钟



# 6. 分布式锁

分布式锁解决的问题是: 在分布式系统下不同进程对于共享资源的访问需要互斥来防止彼此干扰

分布式锁需要具备的特点:

- **互斥性**: 任意时刻，只有一个客户端能持有锁。
- **锁超时释放**：持有锁超时，可以释放，防止不必要的资源浪费，也可以防止死锁。
- **可重入性**:一个线程如果获取了锁之后,可以再次对其请求加锁。
- **高性能和高可用**：加锁和解锁需要开销尽可能低，同时也要保证高可用，避免分布式锁失效。
- **安全性**：锁只能被持有的客户端删除，不能被其他客户端删除

## 6.1 Redis

> https://juejin.cn/post/6936956908007850014#heading-2

**目的**：使用分布式锁来限制程序的并发执行。

**格式**：用lua脚本实现加锁和解锁的的原子性

```sh
// 原子性加锁并设置过期时间
set <key> true ex <seconds> nx
del <key>
```

**演化：**

- 开始是setnx <key> true(加锁操作)
- 但是如果执行过程中出现了异常，那么这个锁就永远得不到释放；所以需要添加一个过期事件expire <key> <seconds>
- 但是如果setnx 和 expire执行之间出现了错误，那么还是会出现死锁情况。所以setnx 和 expire操作应该是原子性的
- 使用事务是不行的，因为Redis的事务不是原子性的，会出现没抢到锁还是执行了expire

**超时问题**：

- 因为业务执行流程较长，使得到了过期时间而业务代码未执行完就自动释放锁
- 守护线程解决方案：额外起一个线程，定期检查线程是否还持有锁，如果有则延长过期时间。有和引入redlock相同的问题
- 超时回滚解决方案：当我们解锁时发现锁已经被其他线程获取了，说明此时我们执行的操作已经是“不安全”的了，此时需要进行回滚，并返回失败

**可重入性**： Java中需要配合ThreadLocal来实现可重入性

**加锁失败**：因为获取不到Redis分布式锁, 不会进入阻塞状态, 本质上是一个boolean判断

- `直接抛出异常，通知客户端稍后重试`：适合于由用户直接发起的请求，用户看到错误后，自己点重试，起到人工延时的作用
- `sleep以后然后重试`：会阻塞当前的消息处理线程，容易造成消息处理有延时
- `将请求转移至延时队列中，过一会再试`：将消息序列化作为zset的value，过期时间作为score，放入到zset中；多个线程轮询zset获取到期的任务进行处理。

**RedLock算法**: 工业上有争议

- `出现原因`：在集群模式下，可能出一个客户端在主节点申请一把锁，但是这个锁还未同步到从节点，主节点就挂掉了；导致另一个客户端请求加锁时，新的主节点立刻就批准了；导致一把锁背多个客户端持有
- 需要提供多个Redis实例，加锁时向过半结点发送set(key,valie,nx = True,ex = xxx)指令，只要过半结点set成功，则认为加锁成功

---

## 6.2 Zookeeper

Zookeeper方案：安全性更好

1、创建一个锁目录 /locks，该节点为持久节点

2、想要获取锁的线程都在锁目录下创建一个临时顺序节点

3、获取锁目录下所有子节点，对子节点按节点自增序号从小到大排序

4、判断本节点是不是第一个子节点，如果是，则成功获取锁，开始执行业务逻辑操作；如果不是，则监听自己的上一个节点的删除事件

5、持有锁的线程释放锁，只需删除当前节点即可。

6、当自己监听的节点被删除时，监听事件触发，则回到第3步重新进行判断，直到获取到锁。



