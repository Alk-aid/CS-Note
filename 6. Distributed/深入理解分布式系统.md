# 1. 认识分布式系统

什么是分布式系统

- 定义：一个 组件分布在不同的，联网的计算机上, 组件之间通过传递消息进行通信和协调，共同完成一个任务
- 特点：多进程, 不共享操作系统, 不共享时钟
- 优点: 高性能, 可扩展性, 高可用性
- 缺点： 缺乏全局时钟：很难定义两个事件的先后顺序

拜占庭将军模型: 

- 多个将军率领一支军队想要占领一座城市, 军队之间只能通过信使进行交流
- 军队的动作只有进攻和撤退, 将军们必须对行为达成一致才算成功
- TODO

分布式系统中的同步 和 异步：

- 同步：一个消息的响应时间在一个有限且已知的时间范围内
- 异步：一个消息的响应时间是无限的，无法知道一条消息什么时候会到达
- 部分同步：假设系统在大部分时间都是同步的，但偶尔会因为故障转变为异步系统

消息传递语义：

- At Most Once：消息最多传递一次，消息可能丢失，但不会重复
- At Least Once: 系统保证每条消息至少会发送一次，消息不会丢失，但在有故障的情况下可能导致消息重复发送
- Exactly Once: 消息只会被精确传递一次, `难以实现`

# 2. 分布式数据基础

## 2.1 分区(Partition)

> 提高可用性

1. 定义： 将一个数据集拆分为多个较小的数据集, 同时将存储和处理这些较小数据集的责任分配给分布式系统中的不同节点
2. 分类：水平分区 和 垂直分区

- 垂直分区: 对表的列进行拆分; 可以将不常用的列 或者包含text等大对象的列进行拆分
- 水平分区(Sharding,分片): 对表的行进行拆分

3. 水平分区算法：计算某个数据应该划分到哪个分区上

- `范围分区`(Range Partitioning): 根据指定的关键字将数据集拆分为若干连续的范围, 每个范围存储到一个单独的节点上
  - 优点: 实现简单；通过修改范围边界减少范围数据, 能够有效地重新分区
  - 缺点: 无法使用`分区键` 之外的其他关键字进行范围查询； 可能产生数据分布不均匀 或 请求流量不均匀的情况
- `哈希分区`(Hash Partitioning): 将制定的关键字通过hash函数, 根据得到的值来决定该数据集的分区
  - 优点: 数据的分布是随机的，分布较为均匀
  - 缺点: 不支持范围查询；当机器数增加或者减少时，那么映射关系就发生了改变(举例)，从而导致大量缓存的key失效，造成缓存雪崩
- `一致性哈希`(Consistent Partitioning):特殊的hash分区算法

4. 一致性哈希具体介绍：

- 流程: 通过hash环来实现，hash环的大小是$$2^{32}-1$$;；首先将服务器(用IP或者主机名)通过hash映射放到hash环上；然后再将数据也通过hash映射放到hash环上面；然后在hash环上进行顺时针查找距离这个对象最近的一个服务器，让这个服务器缓存这个数据即可
- 优势: 良好的扩展性；当增加了服务器或者减少了，会导致一小部分缓存失效，大部分的缓存key仍然可用
- 缺点：不额外存储数据, 依然不支持范围查询
- hash偏斜问题: 如果服务器没有均匀的分布在hash环上，导致缓存大部分分布在一个服务器上，导致缓存不均匀，使得服务器没有得到良好的使用，一旦缓存失效，容易引起系统的崩溃
- hash偏斜解决: 引入虚拟结点, 将一个真实结点映射为多个虚拟结点。从而缓存读写的流程变成(先找到虚拟结点，再找到真实结点然后进行读取)

5. 分区的挑战: 

- 不好实现事务
- 垂直查询存在多join查询, 很低效

## 2.2 复制(Replication)

1. 定义：将同一份数据冗余存储在多个节点上, 节点通过网络来同步数据, 使之保持一致

2. 优点：提高可用性和安全性; 减少往返时间(用户从最近的副本中拿数据); 增加吞吐量(多个提供读写的机器)

3. 缺点：保持多副本的一致性，增加了复杂性
4. 分类：单主(Single-Master)复制; 多主(Multi-Master)复制; 无主(Leaderless)复制

5. 单主复制: 主从复制, 一主多从

- 写请求必须发送给主节点; 从节点只能处理读请求, 并从主节点同步最新的数据
- `同步复制`: 主节点执行完写请求后, 必须等所有的从节点都执行完毕, 并收到确认信息后才能回复客户端写入成功; `存在性能低下的情况`
- `异步复制`: 主节点执行完写请请求后, 立即将结果返回给客户端, 无须等待其他副本是否写入完成；`存在数据不一致的情况`
- `半同步复制`: 主节点只要等待至少一个从节点返回完成信息即可, 这样保证至少有两个节点拥有最新的数据
- 单主复制优点：只有主节点执行并发写操作,能够保证操作的顺序性, 更加容易支持事物操作
- 单主复制缺点：当主节点宕机, 如果是自动切换的情况, 可能有两个从节点升级为主节点, 导致脑裂

5. 多主复制: 多个节点充当主节点的数据复制方式

- 问题: 多主复制不止一个节点处理写请求, 且网络存在延迟, 这就意味着节点可能会对某些请求的正确顺序产生分歧
- `客户端解决冲突`: 用户自行解决, 将冲突的数据全部返回给客户端, 客户端选择合适的数据返回给系统; 如购物车出现了被删除的物品等
- `最后写入胜利(Last Write Wins)`: 让系统中的每个节点为每个写入请求标记上一个唯一的时间戳或者ID, 冲突发生时选择最新的时间戳; `但是分布式系统难有一个统一的全局时间`
- `因果关系追踪`: 使用算法来跟踪不同请求之间的因果关系
- 多主复制优点: 增加主节点容错性; 多主节点执行写请求, 分担负载的压力
- 多主复制缺点: 复杂性太高 远超他的好处

5. 无主复制: 没有主节点

- 客户端向多个节点发送写请求, 一旦得到其中一些节点的确认响应, 就认为这次写请求成功
- 客户端的读请求也会向多个节点读, 然后获取节点上的数据的版本号, 来决定使用哪一个

## 2.3 CAP & BASE

> CAP理论十二年回顾: 规则变了

`CAP理论`: 在一个异步网络环境中, 对于一个分布式读写存储系统, 不可能同时满足一致性(Consistency), 可用性(Availability), 分区容错性(Partition tolerance)

| 选项            | 描述                                                         |
| --------------- | ------------------------------------------------------------ |
| C(Consistence)  | 服务器层面：数据在多个副本之间能够保持一致的特性（强一致性)<br>客户端层面：用户访问分布式系统中的任意节点，得到的数据必须一致(强一致性) |
| A(Availability) | 每次请求都能获取非错误的响应, 但不能保证获取的数据是最新的   |
| P(Partition)    | 不同的节点分布在不同的子网络中，由于一些特殊的原因，这些子节点之间出现了网络不通的状态，但他们的内部子网络是正常的。从而导致了整个系统的环境被切分成了若干个孤立的区域 |
| T（Tolerance)   | 系统在遇到任何网络分区问题时，仍能对外提供服务，除非整个网络都故障了 |

CAP难以同时满足，只能由 CP 或者 AP模式

- 分布式系统节点通过网络连接，必然有网络断开的风险，一定会出现分区问题（P）
- 放弃A：当遇到网络分区或者其他故障时，在此期间无法对外提供正常服务
- 放弃C：放弃C不是放弃一致性，而是放弃强一致性，保留最终一致性

---

`BASE理论`: 是对CAP的一种解决思路（AP方案的补充），包含三个思想：

- `Basically Available （基本可用）`：分布式系统在出现故障时，允许损失部分可用性，即保证核心可用
  - **响应时间上的损失**: 正常情况下，处理用户请求需要 0.5s 返回结果，但是由于系统出现故障，处理用户请求的时间变为3s 
  - **系统功能上的损失**：正常情况，用户可以使用系统全部功能,但是由于系统访问量突然剧增,系统的部分非核心功能无法使用(降级处理)

- `Soft State（软状态）`：**在一定时间内，允许出现中间状态，比如临时的不一致状态。**

- `Eventually Consistent（最终一致性）`：虽然无法保证强一致性，但是在软状态结束后，最终达到数据一致

> Redis的主从数据就是异步的，因此不满足强一致性，；而是保证最终一致性

最终一致性的实现方案

- **读时修复** : 在读取数据时，检测数据的不一致，进行修复
- **写时修复** : 在写入数据，检测数据的不一致时，进行修复
- **异步修复** : 这个是最常用的方式，通过定时对账检测副本数据的一致性，并修复

## 2.4 一致性模型

1. `线性一致性`: 也叫做强一致性； 保证了C 就保证不了A

- 非严格定义: 分布式系统的所有操作看起来都是原子的, 看起来好像只有一个节点
- 严格定义: 给定一个执行历史, 根据并发操作可以扩展为多个顺序历史, 只要从中找到一个合法的顺序历史, 那么这个执行历史就是线性一致的
- 代价: 实现强一致性的同步原语会增加系统开销; 强一致性最困难的是需要一个全局时钟, 这样才能知道每个节点事件发生的时间和全局顺序
- 现代CPU在访问内存时不保证线性一致性

2. `顺序一致性`: 保证了C 就保证不了A

- 只要球同一个客户端(进程)的操作在排序后保持先后顺序不变, 但不同客户端之间的先后顺序是可以任意改变的
- 和强一致性的区别: 没有全局时钟的限制, 不要求不同客户端之间的顺序一致, 只关注局部的顺序
- 举例: 不关心所有朋友的帖子的顺序, 但对于具体一个朋友, 帖子的顺序性要求保证 
- 现代CPU在默认情况也不保证顺序一致性

3. `因果一致性`: 可以容忍一部分节点发生故障, 还未出现故障的节点仍然可用, 但客户不能将请求发送到不可用的副本节点

- 必须以相同的顺序看到因果相关的操作, 而没有因果关系的并发操作可以被不同的进程以不同的顺序观察到
- 比如发帖和回贴, 发帖一定早于回贴

4. `弱一致性`：这种一致性级别约束了系统在写入成功后，不承诺立即可以读到写入的值，也不承诺多久之后数据能够达到一致，但会尽可能地保证到某个时间级别（比如秒级别）后，数据能够达到一致状态

4. `最终一致性`: 最终一致性是弱一致性的一个特例，系统会保证在一定时间内，能够达到一个数据一致的状态。

## 2.5 总结

分布式系统的设计难点: 

- 出发点是为了高性能: 因为数据密集, 单机不足以支撑所有的需求; 所有需要对数据进行`分区`, 然后放到多台服务器上面
- 容错性的需求 :因为服务器的增多, 使得机器出现故障的现象大大增加; 为了提高容错性, 需要对数据进行`复制` 
- 一致性的需求: 因为复制往往会导致数据不一致, 所以我们需要提高`一致性`, 而提高一致性就代表着更低的性能，和出发点违背





# 3. 分布式共识

1. 共识(Consensus)和一致性(Consistency)

- 共识: 侧重于研究分布式系统中的节点达到共识的过程和算法
- 一致性: 研究副本最终的稳定状态

2. 共识的准确描述

- 一个分布式系统包含n个进程, 每个进程都有初始值, 进程之间互相通信, 设计一种算法使得尽管出现故障, 但进程之间仍能协商出某个不可撤销的最终决定
- 每次执行都满足三个性质: 终止性, 协定性, 完整性
- 终止性(Termination): 所有正确的进程最终都会认同某一个值
- 协定性(Agreement): 所有正确的进程认同的值都是同一个值
- 完整性(Integrity): 如果正确的进程都提倡同一个值v, 那么任何正确进程的最终决定值一定是v

3. FLP不可能定理

- 定义: 在一个完全异步系统中, 即使只有一个节点出现了故障, 也不存在一个算法使系统达到共识
- 举例: 进程可以任意时间返回响应, 因此无法分辨进程是速度慢还是崩溃了, 从而无法在有限时间能达到共识
- 含义: 共识算法不可能同时实现三点: `安全性`, `活性`,`容错性`

4. 安全性 和 活性: 分布式共识算法所需要具备的两个属性

- 安全性(Safety): 所有正确的进程都认同同一个值
- 活性(Liveness): 分布式系统最终会认同某一个值

## 3.1 Raft

1. Raft定义: 保证日志完全相同地复制到多台服务器上, 以实现状态机复制的算法; R{eliable|eplicated|edundant} And Fault-default
2. Raft服务器的状态: 

- `Leader`：负责发起心跳，响应客户端，创建日志，同步日志; 同一时刻最多只有一个正常的leader
- `Follower`：接受 Leader 的心跳和日志同步数据，投票给 Candidate;  只响应来自Leader和Candidate的请求, 不会发送任何请求
- `Candidate`: 用来选举出新的leader, 处于leader 和 follower的中间状态

3. Term

- Raft算法选出leader就意味着进入一个新的任期(Term), Term实际上是一个逻辑时间；每个term都由一个数字来表示任期号, 初始值为0, 单调递增且永不为0
- Raft将分布式系统中的时间划分为一个个不同的任期来解决时序问题; Raft只使用最新任期的信息
- Term分为两部分: 任期开始的选举过程 和 任期正常运行的过程; 
- 每个服务器维护一个currentTerm变量表示服务器知道的最新的term号; currentTerm必须持久化存储

4. RPC调用

- RequestVote RPC: 用于领导者选举
- AppendEntries RPC: leader用来日志复制 和 发送心跳

### 3.1.1 领导者选举

1. 领导者选举的前提:

- 如果一个follower在规定时间(electionTimeout)没收到任期更大的空的 AppendEntries RPC请求(心跳检测), 那么就开始新的一轮选举

2. 领导者选举的过程:

- 节点变为candidate, 目标获得超过半数的选票, 让自己成为新的leader
- 增加自己当前任期变量currentTerm
- 给自己投一票
- 并行向系统其他的节点发送RequestVote RPC索要选票

3. 投票的结果

- 获得超过半数选票, 该节点成为新的`leader`, 然后每隔一段时间向其他节点发送AppendEntries消息作为心跳, 来维持自己的leader身份
- 收到来自领导者的心跳RPC请求, 说明已经有leader了, 那么该节点变为`follwer`
- 经过选举超时时间以后, 前面两种情况都没发生, 也没其他节点获胜, 那么回到第二步, 开启新一轮选举

4. 如何保证安全性: 一个term内只有一个leader被选举出来

- 每个节点在同一term内只能投一次票, 给第一个满足条件的RequestVote请求(`TODO:如何算满足条件`)
- 只有获得超过半数的选票才能成为领导者

5. 如何保证活性: 确保系统最终能选出一个leader

- 问题：如果选举同一时间开始, 然后瓜分选票, 没有任何一个节点过半, 又同一时间超时; 然后开启新一轮循环
- 解决: 节点随机选择超时时间,, 选举超时时间通常在[T,2T] (例如150-300)

```go
// Raft节点的结构体
type Raft struct {
  mu sync.Mutex
  // 服务器状态
  state int
  // 服务器已知的最新任期
  currentTerm int
  // 当前任期投票给哪个服务器
  votteFor int
  heartbeatTime time.TIme
  // 其他信息
  // 状态机的日志
  log[] LogEntry
}
```

```go
type RequestVoteArgs struct {
  // 候选者任期
  Term int
  // 候选者id
  CandidateId int
}
type RequestVoteReply struct {
  // 处理请求节点的任期号, 用于候选者更新自己的任期
  Term int
  // 候选者获得选票时为true
  VoteGranted bool
}
```

```go
// 判断请求是否合理
func (rf *Raft) RequestVote(args *RequestVoteArgs, reply *RequestVoteReply) {
  rf.mu.Lock()
  defer rf.mu.UnLock()
  
  reply.Term = rf.currentTerm
  reply.voteGranted = false
  if args.Term < rf.currentTerm {
    return
  }
  // 如果收到来自更大任期的请求, 则更新自己的curretTerm, 转为follwer
  if args.Term > rf.curretTerm {
    rf.currentTerm = args.Term
    rf.state = Follower
    rf.votedFor = -1
  }
  
  if rf.votedFor == -1 || rf.votedFor == args.CandidateId {
    	rf.votedFor = args.CandidateId
	    reply.voteGranted = true
	    rf.heartbeatTime = time.Now()
  }
  return
}
```

### 3.1.2 日志复制

日志格式: 通过索引 和 任期号唯一标识一条日志记录

- index: 表示该日志条目在整个日志中的位置
- 任期号: 日志条目首次被leader创建时的任期
- 命令: 应用于状态机的命令

日志复制流程:

- 客户端向leader发送命令, 希望该命令被所有状态机执行
- leader先将命令追加到自己的日志中, 确保日志持久化存储
- leader 并行向其他节点发送AppendEntries消息, 等待响应
- 如果收到超过半数节点的回应, 则认为该日志`已提交`; 然后leader执行这个命令, 并向客户端返回响应
- 如果followe宕机或者请求超时, 那么leader将尝试反复发送AppendEntries消息

raft 保证以下两个性质：

- 在两个日志里，有两个 entry 拥有相同的 index 和 term，那么它们一定有相同的 cmd，且 它们前面的 entry 也一定相同
- 如果给定的记录已提交, 那么前面所有的记录也提交了

一致性检查: Raft会通过AppendEntries消息来检测之前的日志entry

- 每个AppendEntries消息请求包含 新日志entry之前一个日志的索引(preLogIndex)和任期(preLogTerm)
- follower收到请求后, 会检查自己最后一个日志的索引和任期是否对的上preLogIndex, preLogTerm

```go
type LogEntry struct {
	  Index int 
  	Term	int
	  Command interface{}
}
type AppendEntriesArgs struct {
  	Term 	int
  	LeaderId 	int
  	preLogIndex int
  	preLogTerm 	int
  	// 需要复制的日志条目, 心跳检测时Entries为空
  	Entries	[]LogEntry
  	// leader已经提交的最大的日志索引, 用于follower提交
  	LeaderCommit int
}
type AppendEntriesReply struct {
  	Term int
  	Succeess bool
}
```

```go
//TODO
func (rf *Raft) AppendEntries(args *AppendEntriesArgs, reply *AppendEntriesReply)
```

### 3.1.3 领导者更替

前提:

- Raft假定leader的日志始终是对的, 所以leader要在运行过程中让所有follower的日志都与其匹配



## 3.2 Paxos(TODO)





# 4. 分布式事务

分布式事务的ACID:

- 持久性: 和单点没区别, 只需要在向客户端响应之前, 将数据存在日志中即可
- 一致性: 一般不讨论, 因为实现一致性是由数据库和应用程序一起保证的
- 原子性: 原子提交
- 隔离性: 并发控制

## 4.1 原子提交---实现原子性

### 4.1.1 2PC

2PC就是将事务的提交过程分为了两个阶段来进行处理

阶段一：提交事务请求(投票阶段)

- 事务询问：协调者向所有参与者发送事务内容，询问是否可以执行事务commit操作，然后等待各参与者的反应
- 执行事务：各参与者节点执行事务，并将undo 和 redo信息记录到事务日志中去
- 参与者反馈：如果参与者成功执行了事务操作，反馈YES,否则反馈NO

阶段二：执行事务提交

- 如果收到的反馈都是YES，则执行事务提交
  - 协调者向所有参与者发送Commit请求
  - 参与者收到Commit请求后，会执行Commit操作，然后执行完后释放占用的资源，参与者完成Commit后向协调者发送ACK
  - 协调者收到所有ACK后，完成事务
- 如果至少有一个返回了No或者超时，则中断事务
  - 协调者向参与者发送Rollback请求
  - 参与者收到RollBack请求后，会利用Undo信息执行事务回滚，回滚完释放占用的资源，然后向协调者发送Ack消息
  - 协调者收到所有的ACK后，完成事务中断

优点：原理简单， 实现方便

缺点：

- 同步阻塞：执行过程中，各个参与者都在等待其他参与者的响应而无法进行其他操作
- 单点问题：协调者出现问题那么就会使得流程无法运转，甚至在阶段二出现问题会使得资源不释放
- 脑裂：如果因为网络问题只有部分参与者收到了commit请求，就会导致数据不一致
- 过于保守：**二阶段提交协议没有设计较为完善的容错机制，任意一个节点是失败都会导致整个事务的失败**

### 4.1.2 3PC



## 4.2 并发控制---实现隔离性

