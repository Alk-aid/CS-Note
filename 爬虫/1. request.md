## 常用请求头

- User-Agent:请求载体的身份标识
- Connection:请求完毕后，是断开连接还是保持连接

## 请求头

- Content-Type:服务器响应回客户端的数据类型



## requests模块

### 定义:

​	python中原生的一款基于网络请求的模块

### 作用:

​	模拟浏览器发请求

### 使用步骤

1. 指定url
2. 基于requests模块发起请求
3. 获取响应数据
4. 数据解析
5. 持久化存储

```python
url = 'https://www.sogou.com/'
Response = requests.get(url)
page_text = response.text
print(page_text)
with open('./sogou.html', 'w', encoding='utf-8') as fp:
	fp.write(page_text)

```

### UI伪装

```python
import requests
from requests.models import Response

if __name__ == "__main__":
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36 Edg/87.0.664.75'
    }
    url = 'https://www.sogou.com/web'
    kw = input("enter a word：")
    param = {
        'query': kw
    }
    response = requests.get(url=url, params=param, headers=headers)
    page_text = response.text
    fileName = kw + '.html'
    with open(fileName, 'w', encoding='utf-8') as fp:
        fp.write(page_text)
    print("爬取成功")

```

### 获取局部信息

抓包工具中的XHR对应AJAX请求，来找到AJAX请求的url

```
import requests
import json
from requests.models import Response

if __name__ == "__main__":
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36 Edg/87.0.664.75'
    }

    word = input('enter a word:')
    data = {
        'kw': word
    }

    post_url = 'https://fanyi.baidu.com/sug'

    response = requests.post(post_url, data, headers)

    dic_obj = response.json()
    print(dic_obj)
    # 持久化存储
    fileName = word + '.json'
    fp = open(file=fileName, mode='w', encoding='utf-8')
    json.dump(obj=dic_obj, fp=fp, ensure_ascii=False)

    print("爬取成功")

```

### 动态加载数据

```python
import requests
import json
from requests.models import Response

if __name__ == "__main__":
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36 Edg/87.0.664.75'
    }

    # ID的爬取
    ID_url = 'http://scxk.nmpa.gov.cn:81/xk/itownet/portalAction.do?method=getXkzsList'
    company = input('请输入企业名字:')
    ID_data = {
        "on": True,
        "page": 1,
        "pageSize": 15,
        "productName": company,
        "conditionType": 2,
        "applyname": '',
        "applysn": ''
    }
    ID_response = requests.post(url=ID_url, data=ID_data, headers=headers)
    ID_obj = ID_response.json()
    id_list = []
    for dic in ID_obj['list']:
        id_list.append(dic['ID'])
    print(id_list)
    print("ID爬取成功")
    # ID爬取结束
    all_detail_List=[]
    detail_url='http://scxk.nmpa.gov.cn:81/xk/itownet/portalAction.do?method=getXkzsById'
    for id in id_list:
        detail_data = {
            'id': id
        }
        detial_response = requests.post(url=detail_url, data=detail_data, headers=headers)
        detail_json = detial_response.json()
        all_detail_List.append(detail_json)
        #持久化存储
        fp=open('./详细数据.json','w',encoding='utf-8')
        json.dump(obj=all_detail_List,fp=fp,ensure_ascii=False,indent=True)

```

## 数据解析

### 正则版本

```python
#!/usr/bin/env python
# -*- coding:utf-8 -*-
import requests
import re
import os
#需求：爬取糗事百科中糗图板块下所有的糗图图片
if __name__ == "__main__":
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36'

    }
    #创建一个文件夹，保存所有的图片
    if not os.path.exists('./qiutuLibs'):
        os.mkdir('./qiutuLibs')
    #设置一个通用的url模板
    url = 'https://www.qiushibaike.com/pic/page/%d/?s=5184961'
    # pageNum = 2

    for pageNum in range(1,3):
        #对应页码的url
        new_url = format(url%pageNum)


        #使用通用爬虫对url对应的一整张页面进行爬取
        page_text = requests.get(url=new_url,headers=headers).text

        #使用聚焦爬虫将页面中所有的糗图进行解析/提取
        ex = '<div class="thumb">.*?<img src="(.*?)" alt.*?</div>'
        img_src_list = re.findall(ex,page_text,re.S)
        # print(img_src_list)
        for src in img_src_list:
            #拼接出一个完整的图片url
            src = 'https:'+src
            #请求到了图片的二进制数据
            img_data = requests.get(url=src,headers=headers).content
            #生成图片名称
            img_name = src.split('/')[-1]
            #图片存储的路径
            imgPath = './qiutuLibs/'+img_name
            with open(imgPath,'wb') as fp:
                fp.write(img_data)
                print(img_name,'下载成功！！！')



```

### bs4

#### bs4数据解析的原理：

- 1.实例化一个BeautifulSoup对象，并且将页面源码数据加载到该对象中
- 2.通过调用BeautifulSoup对象中相关的属性或者方法进行标签定位和数据提取

#### 环境安装：

- pip install bs4
- pip install lxml

#### 如何实例化BeautifulSoup对象：

- 1. 导包

```
from bs4 import BeautifulSoup
```

- 2. 对象的实例化：

       1. 将本地的html文档中的数据加载到该对象中

          ```
           fp = open('./test.html','r',encoding='utf-8')
           soup = BeautifulSoup(fp,'lxml')
          ```

       2. .将互联网上获取的页面源码加载到该对象中
              

          ```
          page_text = response.text
          soup = BeatifulSoup(page_text,'lxml')
          ```

3. 提供的用于数据解析的方法和属性：

- > **1. soup.tagName**:
    >
    > ​	返回的是文档中第一次出现的tagName对应的标签

- > **2. soup.find():**
    >
    > - find('tagName'):等同于soup.div
    > - 属性定位：
    >     - soup.find('div',class_/id/attr='song')

- > **3. soup.find_all('tagName')**:
    >
    > ​     返回符合要求的所有标签（列表）

   - **4. select：**

    返回的是一个列表。

    - **select('某种选择器（id，class，标签...选择器）')**
    - 层级选择器：
        - soup.select('.tang > ul > li > a')：>表示的是一个层级
        - oup.select('.tang > ul a')：空格表示的多个层级

- 5. **获取标签之间的文本数据：**

    - soup.a.text/string/get_text()
        - text/get_text():可以获取某一个标签中所有的文本内容
        - string：只可以获取该标签下面直系的文本内容

   - 6. **获取标签中属性值：**

    - soup.a['href']

注意:不管结果怎么样，select返回的都是**列表**

### xpath

xpath解析：最常用且最便捷高效的一种解析方式。通用性。

> - **xpath解析原理：**
>     - 1.实例化一个etree的对象，且需要将被解析的页面源码数据加载到该对象中。
>     - 2.调用etree对象中的xpath方法结合着xpath表达式实现标签的定位和内容的捕获。
> - **环境的安装：**
>     - pip install lxml
> - **如何实例化一个etree对象:from lxml import etree**
>     - 1.将本地的html文档中的源码数据加载到etree对象中：
>         etree.parse(filePath)
>     - 2.可以将从互联网上获取的源码数据加载到该对象中
>         etree.HTML(page_text)
>     - xpath('xpath表达式')
> - **xpath表达式:** 返回的是列表
>     - /:表示的是从根节点开始定位。表示的是一个层级。
>     - //:表示的是多个层级。可以表示从任意位置开始定位。
>     - 属性定位：//div[@class='song'] tag[@attrName="attrValue"]
>     - 索引定位：//div[@class="song"]/p[3] **索引是从1开始的。**
>     - 取文本：
>         - /text() 获取的是标签中直系的文本内容
>         - //text() 标签中非直系的文本内容（所有的文本内容）
>     - 取属性：
>         /@attrName     ==>img/src